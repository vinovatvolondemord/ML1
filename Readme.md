- ![Нелинейные методы восстановления регрессии](https://github.com/vinovatvolondemord/ML1/blob/master/Readme.md#%D0%BD%D0%B5%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B5-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B-%D0%B2%D0%BE%D1%81%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%B8)
- ![Логические алгоритмы классификации](https://github.com/vinovatvolondemord/ML1/blob/master/Readme.md#%D0%BB%D0%BE%D0%B3%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8)
  - ![Понятие информативности]()
  - ![Методы поиска информативных закономерностей]()
  - ![Решающие списки]()
  - ![Решающие деревья]()
# Нелинейные методы восстановления регрессии
Общая идея в том, что задача сводится к решению последовательности более простых линейных задач.
Пусть задана нелинейная модель регрессии ![](https://latex.codecogs.com/gif.latex?f%28x%2C%5Calpha%29) и требуется минимизировать функционал качества по вектору параметров ![](https://latex.codecogs.com/gif.latex?%5Calpha%5Csubset%5Cmathbb%7BR%7D%5Ep):

![](https://latex.codecogs.com/gif.latex?Q%28%5Calpha%2CX%5El%29%3D%5Csum_%7Bi%3D1%7D%5El%28f%28x_i%2C%5Calpha%29-y_i%29%5E2) .

Для выполнения численной минимизации функционала Q воспользуемся методом Ньютона–Рафсона. Выберем начальное приближение ![](https://latex.codecogs.com/gif.latex?%5Calpha%5E0%3D%5C%7B%5Calpha_1%5E0%2C...%2C%5Calpha_p%5E0%20%5C%7D)и организуем итерационный процесс:

![](https://latex.codecogs.com/gif.latex?%5Calpha%5E%7Bt&plus;1%7D%3A%3D%5Calpha%5Et%20-%20h_t%28Q%27%27%28%5Calpha%5Et%29%29%5E%7B-1%7DQ%27%28%5Calpha%5Et%29%2C)

где ![](https://latex.codecogs.com/gif.latex?Q%27%28%5Calpha%5Et%29) — градиент функционала Q в точке ![](https://latex.codecogs.com/gif.latex?%5Calpha%5Et) ,![](https://latex.codecogs.com/gif.latex?Q%27%27%28%5Calpha%5Et%29)  — гессиан(матрица вторых производных) функционала Q в точке ![](https://latex.codecogs.com/gif.latex?%5Calpha%5Et),![](https://latex.codecogs.com/gif.latex?h_t)— величина шага,который можнорегулировать,а в простейшем варианте просто полагать равным единице.
  Запишем компоненты градиента:
  
  ![](https://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Cpartial%20%7D%7B%5Cpartial%20%5Calpha_j%20%7DQ%28%5Calpha%29%3D2%5Csum%20_%7Bi%3D1%7D%5El%28f%28x_i%2C%5Calpha%20%29-y_i%29%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Calpha%20_j%7D%28x_i%2C%5Calpha%29)

Запишем компоненты гессиана:

![](https://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Cpartial%5E2%20%7D%7B%5Cpartial%20%5Calpha_j%5Cpartial%20%5Calpha%20_k%20%7DQ%28%5Calpha%29%3D2%5Csum%20_%7Bi%3D1%7D%5El%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Calpha_j%7D%28x_i%2C%5Calpha%20%29%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Calpha%20_k%7D%28x_i%2C%5Calpha%20%29-2%5Csum%20_%7Bi%3D1%7D%5El%28f%28x_i%2C%5Calpha%20%29-y_i%29%5Cfrac%7B%5Cpartial%5E2%20f%20%7D%7B%5Cpartial%20%5Calpha_j%5Cpartial%20%5Calpha%20_k%20%7D%28x_i%2C%5Calpha%20%29)


Поскольку функция f задана,градиент и гессиан легко вычисляются численно. Основная сложность метода Ньютона–Рафсона заключается в обращении гессианана каждой итерации.

Более эффективной с вычислительной точки зрения является следующая модификация этого метода.Если функция f достаточно гладкая(дважды непрерывно дифференцируема),то её можно линеаризовать в окрестности текущего значения вектора коэффициентов ![](https://latex.codecogs.com/gif.latex?%5Calpha%20_t):

![](https://latex.codecogs.com/gif.latex?f%28x_i%2C%5Calpha%20%29%3D%20f%28x_i%2C%5Calpha%5Et%29&plus;%5Csum_%7Bj%3D1%7D%5Ep%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Calpha%20_j%7D%28x_i%2C%5Calpha_j%29%28%5Calpha_j-%5Calpha_j%5Et%29)

Заменим в гессиане функцию f на её линеаризацию.Это всё равно,что положить второе слагаемое в гессиане равным нулю.Тогда не нужнобудет вычислятьвторые производные. Этот метод называют методом Ньютона–Гаусса.В остальном он ничем не отличается от метода Ньютона–Рафсона.

Основная сложность метода Ньютона–Рафсона заключается в обращении гессиана на каждой итерации.

Введём матричные обозначения: ![](https://latex.codecogs.com/gif.latex?F_t%3D%28%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Calpha%20_j%20%7D%28x_i%2C%5Calpha%20%5Et%29%20%29%5E%7Bj%3D1%2Cp%7D_%7Bi%3D1%2Cl%7D) — матрица первых производных.

Тогда формула t-й итерации метода Ньютона–Гаусса в матричной записи примет вид:

![](https://latex.codecogs.com/gif.latex?%5Calpha%20%5E%7Bt&plus;1%7D%3A%3D%5Calpha%20%5Et-h_t%28F%5ET_tF_t%29%5E%7B-1%7DF%5ET_t%28f%5Et-y%29)

В правой части записано решение стандартной задачи многомерной линейной регрессии ![](https://latex.codecogs.com/gif.latex?%7C%7CF_t%5Cdelta%20-%28f%5Et-i%29%7C%7C%5E2%5Crightarrow%20%5Cunderset%7B%5Cdelta%7D%7Bmin%7D).Таким образом, в методе Ньютона–Гаусса нелинейная регрессия сводится к последовательности линейных регрессионных задач.Скорость сходимости у него практически такая же, как и у метода Ньютона–Рафсона(оба являются методами второго порядка), но вычисления несколько проще и выполняются стандартными методами линейной регрессии.

# Логические алгоритмы классификации
  Пусть ![](https://latex.codecogs.com/gif.latex?%5Cvarphi%3A%20X%5Crightarrow%20%7B0%2C1%7D) - некоторый предикат, определённый на множестве объектов X. Говорят, что предикат ![](https://latex.codecogs.com/gif.latex?%5Cvarphi) выделяет или покрывает(cover) объект x, если ![](https://latex.codecogs.com/gif.latex?%5Cvarphi%28x%29%3D1) . Предикат называют закономерностью, если он выделяет достаточно много объектов какого-то одного классаc, и практически не выделяет объекты других классов 
  
  Особую ценность представляют закономерности, которые описываются простой логической формулой. Их называют правилами(rules). Процесс поиска правил по выборке называют извлечением  знаний  из  данных(knowledge discovery). К знаниям предъявляется особое требование - они должны быть интерпретируемы, то есть понятны людям. На практике логические закономерности часто ищут в виде конъюнкций небольшого числа элементарных высказываний. Именно в такой форме люди привыкли выражать свой житейский и профессиональный опыт
  
  Всякая закономерность классифицирует лишь некоторую часть объектов. Объединив определённое количество закономерностей в композицию, можно получить алгоритм, способный классифицировать любые объекты. Логическими алгоритмами классификации будем называть композиции легко интерпретируемых закономерностей. При построении логических алгоритмов возникают три основных вопроса:
  
  - Каков критерий информативности, позволяющий называть предикаты закономерностями?
  - Как строить закономерности?
  - Как строить алгоритмы классификации на основе закономерностей?
  
  Напомним основные обозначения. Имеется пространство объектов X и конечное множество имён классов ![](https://latex.codecogs.com/gif.latex?Y%3D%5C%7B1%2C...%2CM%5C%7D) . Целевая зависимость ![](https://latex.codecogs.com/gif.latex?y%5E*%3AX%5Crightarrow) известна только на объектах обучающей выборки ![](https://latex.codecogs.com/gif.latex?X%5El%20%3D%20%28x_i%2Cy_i%29%5El_%7Bi%3D1%7D%2C%20y_i%3Dy%5E*%28x_i%29) . Tребуется построить алгоритм классификаци ![](https://latex.codecogs.com/gif.latex?a%20%3A%20X%5Crightarrow%20Y) ,аппроксимирующий ![](https://latex.codecogs.com/gif.latex?y%5E*) на всём X.
  
  ## Понятие информативности
  **Опр. 1.1.** Предикат ![](https://latex.codecogs.com/gif.latex?%5Cvarphi%28x%29) будем называть логической ε,δ-закономерностью для класса ![](https://latex.codecogs.com/gif.latex?c%5Cin%20Y) , если ![](https://latex.codecogs.com/gif.latex?E&plus;c%28%5Cvarphi%20%2C%20X%5El%29%5Cleq%20%5Cvarepsilon%20%5C%20u%20%5C%20D_c%28%5Cvarphi%2C%20X%5El%29%5Cgeq%20%5Cdelta) при заданных достаточно малом ε и достаточно большом δ из отрезка [0,1].
  
  Eсли ![](https://latex.codecogs.com/gif.latex?n_c%28%5Cvarphi%20%29%3D0) , то закономерность ![](https://latex.codecogs.com/gif.latex?%5Cvarphi) называется чистой или непротиворечивой. Если ![](https://latex.codecogs.com/gif.latex?n_c%28%5Cvarphi%20%29%20%3E0) , то закономерность ![](https://latex.codecogs.com/gif.latex?%5Cvarphi) называется частичной.
